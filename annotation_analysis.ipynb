{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = 'dareia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import read_excel\n",
    "pd.options.display.max_colwidth = 100\n",
    "import re\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "import itertools\n",
    "import textwrap\n",
    "import seaborn as seabornInstance;  seabornInstance.set()\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.dpi']= 500\n",
    "mpl.rc(\"savefig\", dpi=500)\n",
    "plt.rcParams['figure.figsize'] = [7, 20]\n",
    "patterns = [\"|\", \"\\\\\", \"/\", \"+\", \"-\", \".\", \"*\", \"x\", \"o\", \"O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from adjustText import adjust_text\n",
    "def plot_scatter(df, x_values, y_values, labels, filename=None, a=0.4, c='blue', m='o'):\n",
    "    #plt.clf()\n",
    "    df.plot.scatter(ax=ax, y=y_values, x=x_values, alpha=a, c=c, marker=m, s=80)\n",
    "    if labels==None:\n",
    "        pass\n",
    "    else:\n",
    "        texts = []\n",
    "        for x, y, s in df[[x_values, y_values, labels]].itertuples(index=False, name=None):\n",
    "            texts.append(plt.text(x, y, s))\n",
    "        adjust_text(texts) #, x=x_values, y=y_values, autoalign='y',\n",
    "                #only_move={'points':'y', 'text':'y'}, force_points=0.15,\n",
    "                #arrowprops=dict(arrowstyle=\"->\", color='r', lw=0.5))\n",
    "    if filename == None:\n",
    "        filename = 'confidence analysis/{value1}_and_{value2}_scatter.png'.format(value1=y_values, value2=x_values)\n",
    "    plt.savefig(filename, bbox_inches='tight',\n",
    "                #transparent=True\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printsep(s):\n",
    "    print(s*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_ratings(ratings):\n",
    "    printsep('+')\n",
    "    print(\"Normalizing ratings...\")\n",
    "    print(\"shape:\", str(ratings.shape))\n",
    "    ratings = ratings.dropna()\n",
    "    for i in ratings.index:\n",
    "        for y in range(ratings.shape[1]):\n",
    "            if len(str(ratings.iat[i, y])) > 3:\n",
    "                ratings.iat[i, y] = int(ratings.iat[i, y].split(': ')[0])\n",
    "    print(str(ratings))\n",
    "    print(\"... ratings normalized\")\n",
    "    printsep('+')\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotator_probability(maxmeanings, probs):\n",
    "    ratings = [0, 1, 2, 3, 4]\n",
    "    combinations = []\n",
    "    pattern_probability = {}\n",
    "    for i in range(2,maxmeanings+1):\n",
    "        combinations.extend([list(p) for p in itertools.combinations_with_replacement(ratings, i)])\n",
    "    for pattern in combinations:\n",
    "        probability = 1\n",
    "        for elem in pattern:\n",
    "            try:\n",
    "                probability *= probs[elem]\n",
    "            except KeyError:            # this handles the cases where an annotator never used a rating\n",
    "                probability *= 0\n",
    "        pattern_probability[str(pattern)] = probability\n",
    "    return pattern_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data from spreadsheets (.xslx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.path.join(\"..\", \"..\", \"Annotated data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [f for f in os.listdir(directory) if f[-5:]=='.xlsx' and f[0]!='~']\n",
    "print(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheets = []\n",
    "single_words = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This cell runs the loop on one file.\n",
    "for path in paths:\n",
    "    printsep('-|-|')\n",
    "    print('Working with the file ' + path)\n",
    "    s = {} # will write all information from the spreadsheet to this dict\n",
    "    split = re.split('_', path)\n",
    "    s['annotator key'] = split[3]\n",
    "    s['word'] = split[2]\n",
    "    # OPEN FILE\n",
    "    df = read_excel(os.path.join(directory, path),\n",
    "                                  sheet_name=0, encoding='utf-8', dtype=object)\n",
    "    df.columns = [name.lower() for name in df.columns]\n",
    "    df.replace('', np.NaN, inplace=True)\n",
    "    df = df.dropna(how='all')\n",
    "    print(df.tail())\n",
    "    #s['file'] = df # not sure i need that\n",
    "    # GET MEANINGS AND THEIR NUMBER\n",
    "    column_headings = list(df.columns)\n",
    "    print(column_headings)\n",
    "    try:\n",
    "        x = column_headings.index('right context')+1 # where metadata and test sentence ends\n",
    "        y = column_headings.index('comments') # the last column (also checks whether it is \"comments\")\n",
    "        meanings = [m.replace('\\xa0', ' ') for m in column_headings[x:y]] # dealing with the \\xa0 character\n",
    "    except ValueError:\n",
    "        print(\"No such columns in the spreadsheet.\")\n",
    "        meanings = []\n",
    "        break\n",
    "    print('Extracted meanings are:')\n",
    "    print(meanings)\n",
    "    s['meanings'] = meanings\n",
    "    #add number of meanings to the dictionary\n",
    "    s['number of meanings'] = len(meanings)\n",
    "    #print(s['cells'])\n",
    "    # FIND COMMENTS, COUNT THE NUMBER\n",
    "    comments = df.dropna()\n",
    "    print(comments)\n",
    "    s['comments'] = comments\n",
    "    s['number of comments'] = comments.shape[0]\n",
    "    # SELECT A SUBSET WITH ONLY MEANINGS AND RATINGS + INDEX\n",
    "    df_ratings = df.iloc[:, x:y]\n",
    "    df_ratings = normalize_ratings(df_ratings)\n",
    "    s['rows'] = df_ratings.shape[0]\n",
    "    s['cells'] = s['rows'] * s['number of meanings']\n",
    "    print(df_ratings.head())\n",
    "    printsep('-')\n",
    "    # COUNT UNIQUE PATTERNS OF ANNOTATION\n",
    "    all_rating_patterns = []\n",
    "    for i in range(df_ratings.shape[0]):\n",
    "        pattern = str((sorted(df_ratings.iloc[i].to_list())))\n",
    "        all_rating_patterns.append(pattern)\n",
    "    unique_rating_patterns = set(all_rating_patterns)\n",
    "    rating_patterns_counted = {}\n",
    "    for p in all_rating_patterns:\n",
    "        if p in rating_patterns_counted.keys():\n",
    "            rating_patterns_counted[p] += 1\n",
    "        else:\n",
    "            rating_patterns_counted[p] = 1\n",
    "    #pp.pprint(rating_patterns_counted)\n",
    "    #print(rating_patterns_counted)\n",
    "    # turn dictionary into a Series, then DataFrame \n",
    "    rating_patterns_counted = pd.Series(rating_patterns_counted)\n",
    "    rating_patterns_counted = pd.DataFrame({'pattern': rating_patterns_counted.index, 'count': rating_patterns_counted.values})      # result\n",
    "    print(\"Unique rating patterns:\")\n",
    "    pp.pprint(rating_patterns_counted)\n",
    "    s['rating patterns'] = rating_patterns_counted\n",
    "    printsep('-')\n",
    "    # COUNT ALL VALUES ACCROSS THE DATAFRAME\n",
    "    # stack all columns, do value_counts()\n",
    "    arc = df_ratings.stack().value_counts()\n",
    "    # add absolute numbers and their relative frequency to a dataframe\n",
    "    arc = pd.DataFrame({'rating':arc.index,\n",
    "                                        'absolute number':arc.values,\n",
    "                                        'frequency': arc.values / s['cells']})   # result\n",
    "    print(\"All ratings:\")\n",
    "    print(arc)\n",
    "    try:\n",
    "        fours = arc.loc[arc['rating'] == 4, 'absolute number'].item()\n",
    "    except ValueError:\n",
    "        fours = 0\n",
    "    try:\n",
    "        threes = arc.loc[arc['rating'] == 3, 'absolute number'].item()\n",
    "    except ValueError:\n",
    "        threes = 0\n",
    "    try:\n",
    "        twos = arc.loc[arc['rating'] == 2, 'absolute number'].item()\n",
    "    except ValueError:\n",
    "        twos = 0\n",
    "    conf =  fours / (fours + threes + twos)\n",
    "    \n",
    "    s['individual ratings frequency'] = arc\n",
    "    s['confidence'] = conf\n",
    "    s['average'] = (fours * 4 + threes * 3 + twos * 2) / ((fours + threes + twos) * 4)\n",
    "    s['ratings'] = df_ratings\n",
    "    # SHOW RESULTING DICTIONARY\n",
    "    pp.pprint(s)\n",
    "    # ADD THIS WHOLE THING TO THE LIST OF DATA ON SPREADSHEETS\n",
    "    spreadsheets.append(s)\n",
    "    # plot a heatmap of ratings, sorted by date\n",
    "    metadata = df['metadata']\n",
    "    dates = []\n",
    "    for line in metadata:\n",
    "        #print(line)\n",
    "        try:\n",
    "            match = re.search(',cent\\. (.+?),', line)\n",
    "            if match:\n",
    "                datestr = match.group(1)\n",
    "                century = datestr.split()[0]\n",
    "                era = datestr.split()[1]\n",
    "                #print(datestr)\n",
    "                if '-' in century:\n",
    "                    date = (float(century.split('-')[0])+int(century.split('-')[1]))/2\n",
    "                else:\n",
    "                    date = float(century)\n",
    "                if datestr.split()[1] == 'B.':\n",
    "                    date = float(-date)\n",
    "                #print(date)\n",
    "                dates.append(date)\n",
    "            else: \n",
    "                print('ERROR! '*3)\n",
    "                print(line)\n",
    "                while True:\n",
    "                    try:\n",
    "                        date = float(input('Input century manually. For 3 BC, -3; for 10 AD, 10; for 2-3 AD, 2.5 etc.'))\n",
    "                    except ValueError:\n",
    "                        print(\"Not a valid number! Try again.\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        dates.append(date)\n",
    "                        break \n",
    "        except TypeError:\n",
    "            continue \n",
    "    # CREATE A HEATMAP FOR THE CURRENT SPREADSHEET\n",
    "    df_ratings['date: century'] = dates\n",
    "    df_ratings.fillna(value=np.nan, inplace=True)\n",
    "    df_ratings_sorted = df_ratings.sort_values(by=['date: century'])\n",
    "    df_ratings_sorted.index = df_ratings_sorted['date: century']\n",
    "    single_words[s['word']] = df_ratings_sorted\n",
    "    plt.clf()\n",
    "    plt.figure(figsize=(10, 16))\n",
    "    g = seabornInstance.heatmap(df_ratings_sorted.iloc[:,:-1], \n",
    "                            cmap='Blues', \n",
    "                            vmin=0, \n",
    "                            #xticklabels=meanings_short,\n",
    "                            annot=True)\n",
    "    g.set_xticklabels(g.get_xticklabels(), wrap=True)\n",
    "    g.set_xticklabels([textwrap.fill(e.replace('\\xa0', ' '), 15, break_long_words=False) for e in df_ratings_sorted.columns], rotation =30)\n",
    "    plt.title(s['word'])\n",
    "    plt.savefig(\"heatmaps/\"+s['word']+\".png\", bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spreadsheets) # should be 39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_df = pd.DataFrame(spreadsheets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read separately 'virtus' annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtus_paths = [f for f in os.listdir(os.path.join(\"..\", \"..\", \"virtus\")) if f[-5:]=='.xlsx']\n",
    "print(virtus_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtus_spreadsheets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in virtus_paths:\n",
    "    printsep('-|-|')\n",
    "    print('Working with the file ' + path)\n",
    "    s = {} # will write all information from the spreadsheet to this dict\n",
    "    split = re.split('_', path)\n",
    "    s['annotator key'] = split[3][:-6]\n",
    "    s['word'] = split[2]\n",
    "    # OPEN FILE\n",
    "    df = read_excel(os.path.join(\"..\", \"..\", \"virtus\", path),\n",
    "                                  sheet_name=0, encoding='utf-8', dtype=object)\n",
    "    df.columns = [name.lower() for name in df.columns]\n",
    "    df.replace('', np.NaN, inplace=True)\n",
    "    df = df.dropna(how='all')\n",
    "    print(df.tail())\n",
    "    #s['file'] = df # not sure i need that\n",
    "    # GET MEANINGS AND THEIR NUMBER\n",
    "    column_headings = list(df.columns)\n",
    "    print(column_headings)\n",
    "    try:\n",
    "        x = column_headings.index('right context')+1 #where metadata and test sentence ends\n",
    "        y = column_headings.index('comments') #the last column (also checks whether it is \"comments\")\n",
    "        meanings = [m.replace('\\xa0', ' ') for m in column_headings[x:y]] #dealing with the \\xa0 character\n",
    "    except ValueError:\n",
    "        print(\"No such columns in the spreadsheet.\")\n",
    "        meanings = []\n",
    "        break\n",
    "    print('Extracted meanings are:')\n",
    "    print(meanings)\n",
    "    s['meanings'] = meanings\n",
    "    #add number of meanings to the dictionary\n",
    "    s['number of meanings'] = len(meanings)\n",
    "    #print(s['cells'])\n",
    "    # FIND COMMENTS, COUNT THE NUMBER\n",
    "    comments = df.dropna()\n",
    "    print(comments)\n",
    "    s['comments'] = comments\n",
    "    s['number of comments'] = comments.shape[0]\n",
    "    # SELECT A SUBSET WITH ONLY MEANINGS AND RATINGS + INDEX\n",
    "    df_ratings = df.iloc[:, x:y]\n",
    "    df_ratings = normalize_ratings(df_ratings)\n",
    "    s['rows'] = df_ratings.shape[0]\n",
    "    s['cells'] = s['rows'] * s['number of meanings']\n",
    "    print(df_ratings.head())\n",
    "    printsep('-')\n",
    "    # COUNT UNIQUE PATTERNS OF ANNOTATION\n",
    "    all_rating_patterns = []\n",
    "    for i in range(df_ratings.shape[0]):\n",
    "        pattern = str((sorted(df_ratings.iloc[i].to_list())))\n",
    "        all_rating_patterns.append(pattern)\n",
    "    unique_rating_patterns = set(all_rating_patterns)\n",
    "    rating_patterns_counted = {}\n",
    "    for p in all_rating_patterns:\n",
    "        if p in rating_patterns_counted.keys():\n",
    "            rating_patterns_counted[p] += 1\n",
    "        else:\n",
    "            rating_patterns_counted[p] = 1\n",
    "    #pp.pprint(rating_patterns_counted)\n",
    "    #print(rating_patterns_counted)\n",
    "    # turn dictionary into a Series, then DataFrame (there must be a better way, but this works so far...)\n",
    "    rating_patterns_counted = pd.Series(rating_patterns_counted)\n",
    "    rating_patterns_counted = pd.DataFrame({'pattern': rating_patterns_counted.index, 'count': rating_patterns_counted.values})      # result\n",
    "    print(\"Unique rating patterns:\")\n",
    "    pp.pprint(rating_patterns_counted)\n",
    "    s['rating patterns'] = rating_patterns_counted\n",
    "    printsep('-')\n",
    "    # COUNT ALL VALUES ACCROSS THE DATAFRAME\n",
    "    # stack all columns, do value_counts()\n",
    "    arc = df_ratings.stack().value_counts()\n",
    "    # add absolute numbers and their relative frequency to a dataframe\n",
    "    arc = pd.DataFrame({'rating':arc.index,\n",
    "                                        'absolute number':arc.values,\n",
    "                                        'frequency': arc.values / s['cells']})   # result\n",
    "    print(\"All ratings:\")\n",
    "    print(arc)\n",
    "    try:\n",
    "        fours = arc.loc[arc['rating'] == 4, 'absolute number'].item()\n",
    "    except ValueError:\n",
    "        fours = 0\n",
    "    try:\n",
    "        threes = arc.loc[arc['rating'] == 3, 'absolute number'].item()\n",
    "    except ValueError:\n",
    "        threes = 0\n",
    "    try:\n",
    "        twos = arc.loc[arc['rating'] == 2, 'absolute number'].item()\n",
    "    except ValueError:\n",
    "        twos = 0\n",
    "    conf =  fours / (fours + threes + twos)\n",
    "    s['individual ratings frequency'] = arc\n",
    "    s['confidence'] = conf\n",
    "    s['average'] = (fours * 4 + threes * 3 + twos * 2) / ((fours + threes + twos) * 4)\n",
    "    s['ratings'] = df_ratings\n",
    "    # SHOW RESULTING DICTIONARY\n",
    "    pp.pprint(s)\n",
    "    # ADD THIS WHOLE THING TO THE LIST OF DATA ON SPREADSHEETS\n",
    "    virtus_spreadsheets.append(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a 'virtus' heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df_ratings\n",
    "for s in virtus_spreadsheets[:-1]:\n",
    "    temp += s['ratings']\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.fillna(value=np.nan, inplace=True)\n",
    "temp = temp.div(4)\n",
    "plt.clf()\n",
    "plt.figure(figsize=(10, 16))\n",
    "g = seabornInstance.heatmap(temp, \n",
    "                            cmap='Blues', \n",
    "                            vmin=0, \n",
    "                            #xticklabels=meanings_short,\n",
    "                            annot=True\n",
    "                           )\n",
    "g.set_xticklabels([textwrap.fill(e.replace('\\xa0', ' '), 15, break_long_words=False) for e in df_ratings.columns], rotation =30)\n",
    "plt.title(s['word']) # + ' : ' +s['annotator'])\n",
    "plt.savefig(s['word'], bbox_inches='tight') # + ' : ' +s['annotator']+\".png\", )\n",
    "plt.clf()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtus_df = pd.DataFrame(virtus_spreadsheets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# virtus_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation 'styles': grouping data by annotator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataframe columns: 'annotator', 'word', 'rows', 'meanings', 'number of meanings', 'cells', 'comments', 'number of comments', 'rating patterns', 'individual ratings frequency'<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_df['annotator key'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for annotator in cumulative_df['annotator key'].unique():\n",
    "    ann = {}\n",
    "    print('.'*15 + annotator + '.'*15)\n",
    "    ann['annotator'] = annotator\n",
    "    ann_df = cumulative_df[cumulative_df['annotator key'] == annotator]\n",
    "    ann['words annotated'] = len(ann_df)\n",
    "    ann['cells in total'] = sum(ann_df['cells'])\n",
    "# calculate frequency of individual ratings\n",
    "    irf = pd.concat(ann_df['individual ratings frequency'].to_list(), ignore_index=True).iloc[:, :2]\n",
    "    printsep('//')\n",
    "    print(irf)\n",
    "    irf = irf.groupby(['rating']).sum()\n",
    "    irf['frequency'] = irf['absolute number'] / sum(irf['absolute number'])\n",
    "    confidence = irf['frequency'][4]/(irf['frequency'][2]+irf['frequency'][3]+irf['frequency'][4])\n",
    "    average = (irf['absolute number'][3]*3 + irf['absolute number'][2]*2 + irf['absolute number'][4]*4) / ((irf['absolute number'][4]+irf['absolute number'][3]+irf['absolute number'][2])*4)\n",
    "    irf.at[len(irf)+1,'confidence'] = confidence\n",
    "    irf.at[len(irf)+1,'average'] = average\n",
    "    ann['average'] = average\n",
    "    ann[\"confidence\"] = confidence\n",
    "    print(irf)\n",
    "    ann['individual rating frequency'] = irf\n",
    "# calculate individual patterns\n",
    "    prns = pd.concat(ann_df['rating patterns'].to_list(), ignore_index=True)\n",
    "    prns = prns.groupby(['pattern'], as_index=False).sum()\n",
    "    prns['frequency'] = prns['count'] / sum(prns['count'])\n",
    "    ann['patterns'] = prns\n",
    "    print(prns)\n",
    "# calculate sets of patterns\n",
    "    prns['set'] = [str(set(ast.literal_eval(x))) for x in prns['pattern']]\n",
    "    sets = prns.groupby(['set'], as_index=False).sum()\n",
    "    ann['sets'] = sets\n",
    "# append single annotator dictionary to a list of annotators\n",
    "    ann_list.append(ann)\n",
    "    printsep('//')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WRITE ANNOTATOR DATA TO .XLSX\n",
    "for a in ann_list:\n",
    "    with pd.ExcelWriter(a['annotator'] +'_' + str(a['words annotated']) + '_words' '_style_export.xlsx') as writer:\n",
    "        a['individual rating frequency'].to_excel(writer, sheet_name='individual rating frequency')\n",
    "        a['patterns'].to_excel(writer, sheet_name='pattern frequency')\n",
    "        a['sets'].to_excel(writer, sheet_name='sets used')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ann_list) # should be 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE ANNOTATION DATAFRAME\n",
    "annotation_styles = pd.DataFrame(ann_list)\n",
    "# add virtus data if it exists\n",
    "annotation_styles.insert(3, 'confidence: virtus', virtus_df['confidence'])\n",
    "annotation_styles.insert(4, 'average: virtus', virtus_df['average'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_styles.iloc[:, :7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison plots: test task vs. annotation task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.clf()\n",
    "annotation_styles.plot.bar(figsize=(18,5), \n",
    "                           x='annotator', \n",
    "                           y=['confidence: virtus', 'confidence'],\n",
    "                          cmap='Set1',\n",
    "                          )\n",
    "filename = 'virtus-versus-task confidence.png'\n",
    "plt.savefig('confidence analysis/'+filename, bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.clf()\n",
    "annotation_styles.plot.bar(figsize=(18,5), \n",
    "                           x='annotator', \n",
    "                           y=['average: virtus', 'average'],\n",
    "                          cmap='Set2')\n",
    "filename = 'virtus-versus-task average.png'\n",
    "plt.savefig('confidence analysis/'+filename, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional calculation: number of all possible patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counting the number of different patterns / number of possible combinations \n",
    "from scipy.special import comb\n",
    "cumulative_df['count of patterns'] = cumulative_df['rating patterns'].apply(lambda x: x.shape[0])\n",
    "cumulative_df['count of all possible patterns'] = comb(5, cumulative_df['number of meanings'], repetition=True)\n",
    "cumulative_df['% of possible pp used'] = cumulative_df['count of patterns'] / cumulative_df['count of all possible patterns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(cumulative_df.shape[0]):\n",
    "    print(cumulative_df['word'][i])\n",
    "    print('this word has ' + str(cumulative_df['number of meanings'][i]) + ' meanings')\n",
    "    pp.pprint(cumulative_df['meanings'][i])\n",
    "    printsep('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe of confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df = cumulative_df.loc[:, ['annotator key', 'word', 'number of meanings', 'confidence', 'average', 'count of patterns','count of all possible patterns', '% of possible pp used']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annotators_meanings_conf = confidence_df[['annotator key', 'word', 'number of meanings', 'confidence', 'average']]\n",
    "annotators_meanings_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "annotation_styles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis and weighting of the impact of the number of meanings on confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {'A1': '#377eb8', \n",
    "          'A2': '#ff7f00',\n",
    "          'A3': '#4daf4a',\n",
    "          'A4':  '#f781bf',\n",
    "          'A5': '#a65628',\n",
    "          'A6': '#984ea3',\n",
    "          'A7': '#999999',\n",
    "         }\n",
    "\n",
    "'''\n",
    "'#377eb8', '#ff7f00', '#4daf4a',\n",
    "                  '#f781bf', '#a65628', '#984ea3',\n",
    "                  '#999999', '#e41a1c', '#dede00'\n",
    "                  SOURCE: https://gist.github.com/thriveth/8560036\n",
    "'''\n",
    "\n",
    "markers = {'A1': 'o', \n",
    "          'A2': 'x',\n",
    "          'A3': '+',\n",
    "          'A4':  'v',\n",
    "          'A5': '*',\n",
    "          'A6': '^',\n",
    "          'A7': 's',\n",
    "         } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "for k in cumulative_df['annotator key'].unique():\n",
    "    plot_scatter(annotators_meanings_conf.loc[annotators_meanings_conf['annotator key']==k], 'number of meanings', 'confidence', 'annotator key', \n",
    "             filename='confidence analysis/meanings_confidence_annotators.png', \n",
    "             a=1,\n",
    "            c=colors[k],\n",
    "             m=markers[k], \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = cumulative_df.groupby(['number of meanings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_of_meanings = cumulative_df['number of meanings'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanings_and_patterns = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,8): \n",
    "    m = grouped.get_group(i)\n",
    "    meanings_and_patterns[i] = pd.concat([m['annotator key'],m['word'],m['rating patterns'],m['individual ratings frequency']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_meanings = {}\n",
    "average_meanings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in meanings_and_patterns:\n",
    "    # the value is a df\n",
    "    nr_df = meanings_and_patterns[key]\n",
    "    w_and_a = nr_df.iloc[:, 0:2]\n",
    "    prns = pd.concat(nr_df['rating patterns'].to_list(), ignore_index=True)\n",
    "    prns = prns.groupby(['pattern'], as_index=False).sum()\n",
    "    prns['frequency'] = prns['count'] / sum(prns['count'])\n",
    "    irfs = pd.concat(nr_df['individual ratings frequency'].to_list(), ignore_index=True)\n",
    "    irfs = irfs.groupby(['rating'], as_index=False).sum()\n",
    "    irfs['frequency'] = irfs['absolute number']/sum(irfs['absolute number'])\n",
    "    confidence = irfs['frequency'][4]/(irfs['frequency'][2]+irfs['frequency'][3]+irfs['frequency'][4])\n",
    "    average = (irfs['absolute number'][4]*4 + irfs['absolute number'][3]*3 + irfs['absolute number'][2]*2) / ((irfs['absolute number'][2]+irfs['absolute number'][3]+irfs['absolute number'][4])*4)\n",
    "    irfs.at[len(irfs),'confidence'] = confidence\n",
    "    irfs.at[len(irfs),'average'] = average\n",
    "    confidence_meanings[key] = confidence\n",
    "    average_meanings[key] = average\n",
    "    with pd.ExcelWriter('{} meanings.xlsx'.format(key)) as writer:\n",
    "        w_and_a.to_excel(writer, sheet_name='annotators and words')\n",
    "        irfs.to_excel(writer, sheet_name='rating frequency')\n",
    "        prns.to_excel(writer, sheet_name='pattern frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the outlier 'jus' is taken out; confidence '6a' stores the value with the outlier\n",
    "confidence_meanings['6a']=confidence_meanings[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_meanings[6]=0.563003 # average without 'jus'\n",
    "confidence_meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "by_meanings = {}\n",
    "\n",
    "for key in average_meanings.keys():\n",
    "    by_meanings[key]=[average_meanings[key], confidence_meanings[key]]\n",
    "\n",
    "df_by_meanings = pd.DataFrame(by_meanings, index=['meanings average', 'meanings confidence']).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_by_meanings.loc[2, 'meanings confidence'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.clf()\n",
    "df_by_meanings['meanings confidence'].plot(figsize=(18,5),\n",
    "                                      x='number of meanings', \n",
    "                                      y='meanings confidence')\n",
    "filename = 'confidence by number of meanings_new6.png'\n",
    "plt.savefig(filename, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotators_confidence = {dict[\"annotator\"]:dict[\"confidence\"] for dict in ann_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotators_meanings_conf = pd.merge(annotators_meanings_conf, df_by_meanings, left_on='number of meanings', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotators_meanings_conf.sort_index(inplace=True)\n",
    "annotators_meanings_conf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADDING WEIGHTING COEFFICIENT\n",
    "k3 = 1\n",
    "annotators_meanings_conf['coefficient']=1/(annotators_meanings_conf['meanings confidence']/annotators_meanings_conf['meanings confidence'][0])\n",
    "annotators_meanings_conf['meanings confidence'][0]\n",
    "annotators_meanings_conf['weighted'] = annotators_meanings_conf['confidence'] * annotators_meanings_conf['coefficient']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtus_coeff = float(annotators_meanings_conf.loc[annotators_meanings_conf['number of meanings']==6, ['coefficient']].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtus_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_df = pd.DataFrame(index=annotators_confidence.keys())\n",
    "weighted_df['weighted confidence'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for annotator in annotators_confidence.keys():\n",
    "    #print(annotator)\n",
    "    annotator_df = annotators_meanings_conf[annotators_meanings_conf['annotator key'] == annotator]\n",
    "    #print(annotator_df)\n",
    "    weighted_confidence = annotator_df['weighted'].sum()/len(annotator_df)\n",
    "    #print(weighted_confidence)\n",
    "    weighted_df.loc[[annotator], ['weighted confidence']] = weighted_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_df.reset_index(level=0, inplace=True)\n",
    "weighted_df = weighted_df.rename(columns={'index': 'annotator'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_styles = annotation_styles.merge(weighted_df, on='annotator')\n",
    "annotation_styles['weighted confidence: virtus'] = virtus_coeff * annotation_styles['confidence: virtus']\n",
    "annotation_styles[['annotator', 'words annotated', 'cells in total', 'confidence: virtus', 'weighted confidence: virtus', 'confidence', 'weighted confidence']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots: annotators' confidence scores before and after weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.clf()\n",
    "annotation_styles.plot.bar(figsize=(18,5), \n",
    "                           x='annotator', \n",
    "                           y=[\n",
    "                               'confidence: virtus', \n",
    "                               'weighted confidence: virtus',\n",
    "                               'confidence', \n",
    "                               'weighted confidence',\n",
    "                           ])\n",
    "filename = 'confidence analysis/virtus-average-vs-weighted-confidence_new6.png'\n",
    "plt.savefig(filename, bbox_inches='tight', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close()\n",
    "plt.clf()\n",
    "annotation_styles.plot.bar(figsize=(18,5), \n",
    "                           x='annotator', \n",
    "                           y=[\n",
    "                               #'confidence: virtus', \n",
    "                               'confidence', \n",
    "                               #'weighted confidence: virtus',\n",
    "                               'weighted confidence',\n",
    "                           ])\n",
    "filename = 'confidence analysis/average-vs-weighted-confidence_new6.png'\n",
    "plt.savefig(filename, bbox_inches='tight',transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotators_meanings_conf = annotators_meanings_conf.rename(columns={'weighted':'weighted confidence'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plots: scatter plots of all annotated words with weighted confidence values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "for k in cumulative_df['annotator key'].unique():\n",
    "    plot_scatter(annotators_meanings_conf.loc[annotators_meanings_conf['annotator key']==k], 'number of meanings', 'weighted confidence', 'word', \n",
    "             filename='confidence analysis/meanings_weighted confidence_words.png', \n",
    "             a=1,\n",
    "            c=colors[k],\n",
    "             #m=markers[k],                 \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.rcParams['figure.figsize'] = [10, 10]\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "for k in cumulative_df['annotator key'].unique():\n",
    "    plot_scatter(annotators_meanings_conf.loc[annotators_meanings_conf['annotator key']==k], 'number of meanings', 'weighted confidence', 'annotator key', \n",
    "             filename='scatter_plots/meanings_confidence_annotators_weighted.png', \n",
    "             a=1,\n",
    "            c=colors[k],\n",
    "             m=markers[k],\n",
    "                 \n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotators_meanings_conf[['confidence', \n",
    "    'weighted confidence']].plot(kind='box', #title='comparison of non-weighted and weighted confidences'\n",
    "                                )\n",
    "plt.savefig('confidence analysis/weighed-confidences-boxplot-new6.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding new data to the annotation_styles dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have two values: \n",
    "- expected confidence of a specific annotator\n",
    "- expected confidence for a word with N meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "float(annotators_meanings_conf[annotators_meanings_conf['word']=='acerbus']['weighted confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_styles.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df['expected confidence: annotator'] = confidence_df['annotator key'].apply(lambda x: annotators_confidence[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df['expected weighted confidence: annotator'] = confidence_df['annotator key'].apply(lambda x: float(annotation_styles[annotation_styles['annotator']==x]['weighted confidence']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df['virtus confidence: annotator'] = confidence_df['annotator key'].apply(\n",
    "    lambda x: annotation_styles.loc[annotation_styles['annotator']==x, 'confidence: virtus'].values[0])\n",
    "confidence_df['virtus weighted confidence: annotator'] = confidence_df['annotator key'].apply(\n",
    "    lambda x: annotation_styles.loc[annotation_styles['annotator']==x, 'weighted confidence: virtus'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df['expected confidence: nr of meanings'] = confidence_df['number of meanings'].apply(lambda x: confidence_meanings[x])\n",
    "confidence_df['expected average: nr of meanings'] = confidence_df['number of meanings'].apply(lambda x: average_meanings[x])\n",
    "confidence_df['weighted confidence'] = confidence_df['word'].apply(lambda x: float(annotators_meanings_conf[annotators_meanings_conf['word']==x]['weighted confidence']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df['diff to annotator'] = confidence_df['expected confidence: annotator']-confidence_df['confidence']\n",
    "confidence_df['diff to meanings'] = confidence_df['expected confidence: nr of meanings']-confidence_df['confidence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of confidence and the semantic qualities of annotated words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_data = read_excel(os.path.join(\"..\", \"..\", 'Words_qualities.xlsx'), sheet_name=0, encoding='utf-8', dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with the structure of the dictionary entry (Lewis and Short)\n",
    "Data on the meanings was from L&S was collected manually and stored as a string (e.g. 'I, I.B.1, I.B.3, I.B.4, II'), in which each item corresponds to the position of the selected meaning within the hierarchy of the L&S entry for the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lewis_and_short = words_data[['word','L&S hierarchy']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.children = []\n",
    "        \n",
    "    def list_children(self):\n",
    "        if len(self.children) == 0:\n",
    "            return ''\n",
    "        result_string = ': [ '\n",
    "        for child in self.children:\n",
    "            result_string += child.to_string() + ' '\n",
    "        result_string += ']'\n",
    "        return result_string\n",
    "        \n",
    "    \n",
    "    def to_string(self):\n",
    "        return self.name + self.list_children()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_parser(tokens, parent_node): # tokens is a list\n",
    "    if len(tokens) == 0:\n",
    "        return\n",
    "    \n",
    "    child_node = None\n",
    "    for child in parent_node.children:\n",
    "        if child.name == tokens[0]:\n",
    "            child_node = child\n",
    "    if child_node == None:\n",
    "        child_node = Node(tokens[0])\n",
    "        parent_node.children.append(child_node)\n",
    "        \n",
    "    node_parser(tokens[1:], child_node)\n",
    "        \n",
    "        \n",
    "def hierarchy(string):\n",
    "    string = ''.join(string.split())\n",
    "    print(string)\n",
    "    x = string.split(',') # x is a list\n",
    "    print(len(x)) # number of meanings\n",
    "    root = Node('Ã¸')\n",
    "    for i in range(len(x)):\n",
    "        tokens = x[i].split('.') \n",
    "        node_parser(tokens, root)\n",
    "    \n",
    "    print(root.to_string())\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = 'I, I.B.1, I.B.3, I.B.4, II'\n",
    "hierarchy(test_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_splits(node):\n",
    "    print(node.to_string())\n",
    "    explicit_splits = 0\n",
    "    if len(node.children) == 1:\n",
    "        pass\n",
    "    else: \n",
    "        explicit_splits = len(node.children)\n",
    "    \n",
    "    implicit_splits = 0\n",
    "    for child in node.children:\n",
    "        implicit_splits += find_splits(child)\n",
    "    \n",
    "    return explicit_splits + implicit_splits\n",
    "    \n",
    "\n",
    "def tree_complexity(string):\n",
    "    root = hierarchy(string)\n",
    "    print('in tree_complexity())' + root.to_string())\n",
    "    return find_splits(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tree_complexity(test_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = tree_complexity('I, II.A, II.B, II.C.1, II.C.2.a, II.C.2.b, II.B.2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_data['L&S complexity'] = words_data['L&S hierarchy'].map(tree_complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df = pd.merge(left=confidence_df, right=words_data, how='right', left_on=['word', 'number of meanings'], right_on=['word', 'number of meanings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confidence_df[confidence_df['word']=='jus']['expected confidence: nr of meanings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df.to_excel('Confidence comparison_new6.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confidence_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of the relationship between confidence and suggested influencing quality for each words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(confidence_df, 'weighted confidence', 'confidence', 'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(confidence_df, 'expected weighted confidence: annotator', 'confidence', 'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(confidence_df, 'sources (Fr)', 'confidence', 'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(confidence_df, '% of possible pp used', 'confidence', 'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(confidence_df, 'count of patterns', 'confidence', 'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(confidence_df, 'count of patterns', 'average', 'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(confidence_df, 'L&S complexity', 'confidence', 'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(confidence_df, 'L&S complexity', 'number of meanings', 'word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(confidence_df, 'POS', 'confidence', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = confidence_df.plot.scatter(x='confidence', y='word', color='blue', label='real confidence')\n",
    "#confidence_df.plot.scatter(x='expected confidence: annotator', y='word', color='green', ax=ax, label='annotator confidence')\n",
    "#confidence_df.plot.scatter(x='expected confidence: nr of meanings', y='word', color='cyan', ax=ax, label='meanings confidence', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "plt.rcParams['figure.figsize'] = [20, 4]\n",
    "pl = confidence_df.plot.scatter(x='word', y='diff to annotator', color='blue', label='diff to annotator')\n",
    "confidence_df.plot.scatter(x='word', y='diff to meanings', color='red', label='diff to meanings', ax=pl)\n",
    "plt.xticks(rotation=45)\n",
    "plt.axhline(y=0.1, c='gray')\n",
    "plt.axhline(y=-0.1, c='gray')\n",
    "plt.axhline()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plt.rcParams['figure.figsize'] = [20, 4]\n",
    "pl = confidence_df.loc[confidence_df['target or control?_x']=='target'].plot.scatter(x='word', y='confidence', color='green', label='confidence: target words')\n",
    "confidence_df.loc[confidence_df['target or control?_x']=='control'].plot.scatter(x='word', y='confidence', color='black', label='confidence: control words', ax=pl)\n",
    "plt.xticks(rotation=45)\n",
    "plt.axhline(y=0.5, c='gray')\n",
    "#plt.axhline(y=0.75, c='gray')\n",
    "plt.axhline()\n",
    "plt.savefig('words_confidences_target_or_control.png', bbox_inches='tight')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plt.rcParams['figure.figsize'] = [20, 4]\n",
    "pl = confidence_df.loc[confidence_df['target or control?_x']=='target'].plot.scatter(x='word', y='weighted confidence', color='green', label='confidence: target words')\n",
    "confidence_df.loc[confidence_df['target or control?_x']=='control'].plot.scatter(x='word', y='weighted confidence', color='black', label='confidence: control words', ax=pl)\n",
    "plt.xticks(rotation=45)\n",
    "plt.axhline(y=0.5, c='gray')\n",
    "#plt.axhline(y=0.75, c='gray')\n",
    "plt.axhline()\n",
    "plt.savefig('words_confidences_target_or_control_weighted.png', bbox_inches='tight')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 4]\n",
    "pl = confidence_df.plot.scatter(x='word', \n",
    "                                y='weighted confidence', \n",
    "                                color='purple', \n",
    "                                #label='confidence: all words'\n",
    "                               )\n",
    "plt.xticks(rotation=45)\n",
    "#plt.axhline(y=0.5, c='gray')\n",
    "#plt.axhline(y=0.75, c='gray')\n",
    "plt.axhline()\n",
    "plt.savefig('confidence analysis/words_confidences_all_weighted.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 4]\n",
    "pl = confidence_df.plot.scatter(x='word', \n",
    "                                y='confidence', \n",
    "                                color='purple', \n",
    "                                #label='confidence: all words'\n",
    "                               )\n",
    "plt.xticks(rotation=45)\n",
    "#plt.axhline(y=0.5, c='gray')\n",
    "#plt.axhline(y=0.75, c='gray')\n",
    "plt.axhline()\n",
    "plt.savefig('confidence analysis/words_confidences_all_notweighted.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [20, 4]\n",
    "pl = confidence_df.plot.scatter(x='word', \n",
    "                                y='average', \n",
    "                                color='teal', \n",
    "                                #label='confidence: all words'\n",
    "                               )\n",
    "plt.xticks(rotation=45)\n",
    "#plt.axhline(y=0.5, c='gray')\n",
    "#plt.axhline(y=0.75, c='gray')\n",
    "#plt.axhline()\n",
    "plt.savefig('confidence analysis/words_averages_all.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting single words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# single_words is a dictionary of dataframes containing cleaned up annotation data for every word in the task\n",
    "single_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_confidence(row):\n",
    "    context = row.to_list()[:-1]\n",
    "    fours = 0\n",
    "    others = 0\n",
    "    for rating in context:\n",
    "        if rating == 4.0:\n",
    "            fours += 1\n",
    "        if rating == 3.0 or rating == 2.0:\n",
    "            others += 1\n",
    "    try: \n",
    "        return(fours/(fours+others))\n",
    "    except ZeroDivisionError:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_average(row):\n",
    "    context = row.to_list()[:-1]\n",
    "    fours = 0\n",
    "    others = 0\n",
    "    for rating in context:\n",
    "        if rating == 4.0:\n",
    "            fours += 1\n",
    "        if rating == 3.0 or rating == 2.0:\n",
    "            others += 1\n",
    "    try: \n",
    "        return(fours/(fours+others))\n",
    "    except ZeroDivisionError:\n",
    "        return(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def single_word_analysis(analysed_word):    \n",
    "    word_df = single_words[analysed_word]\n",
    "    word_conf = confidence_df.loc[confidence_df['word']==analysed_word]['confidence'].to_list()[0]\n",
    "    word_conf_weighted = confidence_df.loc[confidence_df['word']==analysed_word]['weighted confidence'].to_list()[0]\n",
    "    word_average = confidence_df.loc[confidence_df['word']==analysed_word]['average'].to_list()[0]\n",
    "    word_df.index = range(len(word_df))\n",
    "    word_df['confidence'] = word_df.apply(context_confidence, axis=1)\n",
    "    plt.rcParams['figure.figsize'] = [20, 4]\n",
    "    pl = word_df.plot(y='confidence', linestyle=\"\",marker=\"o\") #(x='date: century', y='confidence', color='purple', label='confidence per context')\n",
    "    #plt.xticks(rotation=45)\n",
    "    plt.axhline(y= word_conf, c='gray')\n",
    "    plt.savefig('words visualised/all contexts visualisation/' + analysed_word +'_all_contexts_visualised.png', bbox_inches='tight')\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_word_diachronic_plot(analysed_word):\n",
    "    print(analysed_word)\n",
    "    word_df=single_words[analysed_word]\n",
    "    plt.rcParams['figure.figsize'] = [20, 4]\n",
    "    pivot = pd.pivot_table(word_df.iloc[:, :-1].replace([1],0),\n",
    "                       index='date: century',\n",
    "                       aggfunc=np.average\n",
    "    )\n",
    "    pivot = pivot.reindex(np.arange(-2, 20.5,0.5))\n",
    "    pivot = pivot.div(pivot.sum(1), axis=0)\n",
    "    pivot.plot(kind='bar', stacked=True)\n",
    "    filename='{0}-diachronic-plot'.format(analysed_word)\n",
    "    plt.savefig('words visualised/diachronic plots/'+filename, bbox_inches='tight')\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_word_diachronic_plot_unstacked(analysed_word):\n",
    "    print(analysed_word)\n",
    "    word_df=single_words[analysed_word]\n",
    "    pivot = pd.pivot_table(word_df.iloc[:, :-1].replace([1],0),\n",
    "                       index='date: century',\n",
    "                       aggfunc=np.average\n",
    "    )\n",
    "    plt.rcParams['figure.figsize'] = [20, 4]\n",
    "    pivot = pivot.reindex(np.arange(-2, 20.5,0.5))\n",
    "    #pivot = pivot.div(pivot.sum(1), axis=0)\n",
    "    pivot.plot(kind='bar', stacked=True)\n",
    "    filename='{0}-diachronic-plot-unstacked'.format(analysed_word)\n",
    "    plt.savefig('words visualised/diachronic plots/'+filename, bbox_inches='tight')\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in single_words.keys():\n",
    "    single_word_diachronic_plot(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in single_words.keys():\n",
    "    single_word_diachronic_plot_unstacked(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in single_words.keys():\n",
    "    single_word_analysis(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking for explanations of confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = confidence_df.plot.scatter(x='confidence', y='expected confidence: nr of meanings', figsize=[7,7])\n",
    "pl.plot([0, 1], [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = confidence_df.plot.scatter(x='confidence', y='expected confidence: annotator', figsize=[7,7])\n",
    "pl.plot([0, 1], [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = confidence_df.plot.scatter(x='expected confidence: nr of meanings', y='expected confidence: annotator', figsize=[7,7])\n",
    "pl.plot([0,2, 1,2], [0,2, 1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #1: based on expected confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = '#1'\n",
    "predictors = ['expected confidence: annotator', 'expected confidence: nr of meanings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = confidence_df[predictors].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = confidence_df['confidence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seabornInstance.distplot(confidence_df['confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()  \n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred}, index=range(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Mean absolute error should be within the 10% range of the mean confidence value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df['confidence'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: R squared is in the interval (0, 1), the greater the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.explained_variance_score(y_test, y_pred) # maximum value is 1 and is desirable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.max_error(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics.median_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add result to a dictionary\n",
    "models_results.append({\n",
    "    'model': model, \n",
    "    'predictors': predictors, \n",
    "    'MAE': metrics.mean_absolute_error(y_test, y_pred),\n",
    "    'MSE': metrics.mean_squared_error(y_test, y_pred),\n",
    "    'RMSE': np.sqrt(metrics.mean_squared_error(y_test, y_pred)),\n",
    "    'r squared': metrics.r2_score(y_test, y_pred),\n",
    "    'explained variance score': metrics.explained_variance_score(y_test, y_pred),\n",
    "    'max error': metrics.max_error(y_test, y_pred)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #1.a: based on expected annotator & meanings confidences +  virtus confidences (only for the 4 annotators who annotated virtus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = '#1.a'\n",
    "predictors = ['virtus confidence: annotator', 'expected confidence: annotator', 'expected confidence: nr of meanings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = confidence_df[predictors].dropna().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = confidence_df[['virtus confidence: annotator', 'expected confidence: annotator', 'expected confidence: nr of meanings', 'confidence']].dropna().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seabornInstance.distplot(confidence_df[['virtus confidence: annotator', 'expected confidence: annotator', 'expected confidence: nr of meanings', 'confidence']].dropna()['confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()  \n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred}, index=range(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Mean absolute error should be within the 10% range of the mean confidence value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df.dropna()['confidence'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: R squared "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.explained_variance_score(y_test, y_pred) # maximum value is 1 and is desirable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.max_error(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics.median_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add result to a dictionary\n",
    "models_results.append({\n",
    "    'model': model, \n",
    "    'predictors': predictors, \n",
    "    'MAE': metrics.mean_absolute_error(y_test, y_pred),\n",
    "    'MSE': metrics.mean_squared_error(y_test, y_pred),\n",
    "    'RMSE': np.sqrt(metrics.mean_squared_error(y_test, y_pred)),\n",
    "    'r squared': metrics.r2_score(y_test, y_pred),\n",
    "    'explained variance score': metrics.explained_variance_score(y_test, y_pred),\n",
    "    'max error': metrics.max_error(y_test, y_pred)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #1.b: based only on number of meanings +  virtus confidences \n",
    "(only for the 4 annotators who did virtus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = '#1.b'\n",
    "predictors = ['number of meanings', 'virtus confidence: annotator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = confidence_df[predictors].dropna().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = confidence_df.dropna()['confidence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X) == len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seabornInstance.distplot(confidence_df.dropna()['confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()  \n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred}, index=range(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Mean absolute error should be within the 10% range of the mean confidence value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df.dropna()['confidence'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: R squared is in the interval (0, 1), the greater the better "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.explained_variance_score(y_test, y_pred) # maximum value is 1 and is desirable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.max_error(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.median_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add result to a dictionary\n",
    "models_results.append({\n",
    "    'model': model, \n",
    "    'predictors': predictors, \n",
    "    'MAE': metrics.mean_absolute_error(y_test, y_pred),\n",
    "    'MSE': metrics.mean_squared_error(y_test, y_pred),\n",
    "    'RMSE': np.sqrt(metrics.mean_squared_error(y_test, y_pred)),\n",
    "    'r squared': metrics.r2_score(y_test, y_pred),\n",
    "    'explained variance score': metrics.explained_variance_score(y_test, y_pred),\n",
    "    'max error': metrics.max_error(y_test, y_pred)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #1.c: based on number of meanings +  virtus confidences + count of patterns\n",
    "(only for the 4 annotators who did virtus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = '#1.c'\n",
    "predictors = ['number of meanings', 'virtus confidence: annotator', 'count of patterns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = confidence_df[predictors].dropna().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = confidence_df.dropna()['confidence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X) == len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seabornInstance.distplot(confidence_df.dropna()['confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()  \n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred}, index=range(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Mean absolute error should be within the 10% range of the mean confidence value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df.dropna()['confidence'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: R squared is in the interval (0, 1), the greater the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.explained_variance_score(y_test, y_pred) # maximum value is 1 and is desirable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.max_error(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.median_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add result to a dictionary\n",
    "models_results.append({\n",
    "    'model': model, \n",
    "    'predictors': predictors, \n",
    "    'MAE': metrics.mean_absolute_error(y_test, y_pred),\n",
    "    'MSE': metrics.mean_squared_error(y_test, y_pred),\n",
    "    'RMSE': np.sqrt(metrics.mean_squared_error(y_test, y_pred)),\n",
    "    'r squared': metrics.r2_score(y_test, y_pred),\n",
    "    'explained variance score': metrics.explained_variance_score(y_test, y_pred),\n",
    "    'max error': metrics.max_error(y_test, y_pred)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model #2: based on (1) annotator confidence; (2) number of meanings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = '#2'\n",
    "predictors = ['expected confidence: annotator', 'number of meanings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = confidence_df[predictors].values\n",
    "#X = confidence_df['expected confidence: annotator'].values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = confidence_df['confidence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seabornInstance.distplot(confidence_df['confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()  \n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred}, index=range(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Mean absolute error should be within the 10% range of the mean confidence value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df['confidence'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: R squared is in the interval (0, 1), the greater the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add result to a dictionary\n",
    "models_results.append({\n",
    "    'model': model, \n",
    "    'predictors': predictors, \n",
    "    'MAE': metrics.mean_absolute_error(y_test, y_pred),\n",
    "    'MSE': metrics.mean_squared_error(y_test, y_pred),\n",
    "    'RMSE': np.sqrt(metrics.mean_squared_error(y_test, y_pred)),\n",
    "    'r squared': metrics.r2_score(y_test, y_pred),\n",
    "    'explained variance score': metrics.explained_variance_score(y_test, y_pred),\n",
    "    'max error': metrics.max_error(y_test, y_pred)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model #3: Based on annotator and number of meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = '#3'\n",
    "predictors = ['annotator', 'number of meanings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotators = pd.get_dummies(cumulative_df['annotator key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([cumulative_df, annotators], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[['A1', 'A2', 'A7', 'A6', 'A3', 'A5', 'A4']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['confidence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()  \n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred}, index=range(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Mean absolute error should be within the 10% range of the mean confidence value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['confidence'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: R squared is in the interval (0, 1), the greater the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add result to a dictionary\n",
    "models_results.append({\n",
    "    'model': model, \n",
    "    'predictors': predictors, \n",
    "    'MAE': metrics.mean_absolute_error(y_test, y_pred),\n",
    "    'MSE': metrics.mean_squared_error(y_test, y_pred),\n",
    "    'RMSE': np.sqrt(metrics.mean_squared_error(y_test, y_pred)),\n",
    "    'r squared': metrics.r2_score(y_test, y_pred),\n",
    "    'explained variance score': metrics.explained_variance_score(y_test, y_pred),\n",
    "    'max error': metrics.max_error(y_test, y_pred)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Interim results: models do not look too good. The best results were achieved by the model based on \"annotator confidence\" and \"meanings confidence\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model #4: based on (1) count of patterns and (2) number of meanings\n",
    "**< Spoiler: the best R-squared so far >**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = '#4'\n",
    "predictors = ['count of patterns', 'number of meanings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = confidence_df[predictors].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = confidence_df['confidence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seabornInstance.distplot(confidence_df['confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()  \n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred}, index=range(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Mean absolute error should be within the 10% range of the mean confidence value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df['confidence'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: R squared is in the interval (0, 1), the greater the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.explained_variance_score(y_test, y_pred) # maximum value is 1 and is desirable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.max_error(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.median_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add result to a dictionary\n",
    "models_results.append({\n",
    "    'model': model, \n",
    "    'predictors': predictors, \n",
    "    'MAE': metrics.mean_absolute_error(y_test, y_pred),\n",
    "    'MSE': metrics.mean_squared_error(y_test, y_pred),\n",
    "    'RMSE': np.sqrt(metrics.mean_squared_error(y_test, y_pred)),\n",
    "    'r squared': metrics.r2_score(y_test, y_pred),\n",
    "    'explained variance score': metrics.explained_variance_score(y_test, y_pred),\n",
    "    'max error': metrics.max_error(y_test, y_pred)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model #5: based on (1) meaning clusters and (2) count of patterns\n",
    "\n",
    "_*NB* the way of counting meaning clusters should be improved_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = '#5'\n",
    "predictors = ['count of patterns', 'meaning clusters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = confidence_df[['count of patterns', 'meaning clusters (D)']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = confidence_df['confidence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()  \n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred}, index=range(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Mean absolute error should be within the 10% range of the mean confidence value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df['confidence'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: R squared is in the interval (0, 1), the greater the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.explained_variance_score(y_test, y_pred) # maximum value is 1 and is desirable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.max_error(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.median_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add result to a dictionary\n",
    "models_results.append({\n",
    "    'model': model, \n",
    "    'predictors': predictors, \n",
    "    'MAE': metrics.mean_absolute_error(y_test, y_pred),\n",
    "    'MSE': metrics.mean_squared_error(y_test, y_pred),\n",
    "    'RMSE': np.sqrt(metrics.mean_squared_error(y_test, y_pred)),\n",
    "    'r squared': metrics.r2_score(y_test, y_pred),\n",
    "    'explained variance score': metrics.explained_variance_score(y_test, y_pred),\n",
    "    'max error': metrics.max_error(y_test, y_pred)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Model #5 performs worse than model #4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model #6: based on (1) number of sources and (2) count of patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = '#6'\n",
    "predictors = ['count of patterns', 'sources (Fr)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = confidence_df[['count of patterns', 'sources (Fr)']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = confidence_df['confidence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()  \n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred}, index=range(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Mean absolute error should be within the 10% range of the mean confidence value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df['confidence'].mean() # mean confidence value = 0.6191 >> MAE should be not greater than 0.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: R squared is in the interval (0, 1), the greater the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.explained_variance_score(y_test, y_pred) # maximum value is 1 and is desirable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.max_error(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.median_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add result to a dictionary\n",
    "models_results.append({\n",
    "    'model': model, \n",
    "    'predictors': predictors,\n",
    "    'additional': 'random_state=42',\n",
    "    'MAE': metrics.mean_absolute_error(y_test, y_pred),\n",
    "    'MSE': metrics.mean_squared_error(y_test, y_pred),\n",
    "    'RMSE': np.sqrt(metrics.mean_squared_error(y_test, y_pred)),\n",
    "    'r squared': metrics.r2_score(y_test, y_pred),\n",
    "    'explained variance score': metrics.explained_variance_score(y_test, y_pred),\n",
    "    'max error': metrics.max_error(y_test, y_pred)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model #6a: based on number of meanings and complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = '#6a'\n",
    "predictors = ['sources (Fr)', 'L&S complexity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = confidence_df[['number of meanings', 'L&S complexity']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = confidence_df['confidence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()  \n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred}, index=range(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Mean absolute error should be within the 10% range of the mean confidence value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df['confidence'].mean() # mean confidence value = 0.6094 >> MAE should be not greater than 0.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: R squared is in the interval (0, 1), the greater the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.explained_variance_score(y_test, y_pred) # maximum value is 1 and is desirable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.max_error(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.median_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add result to a dictionary\n",
    "models_results.append({\n",
    "    'model': model, \n",
    "    'predictors': predictors,\n",
    "    'additional': 'random_state=42',\n",
    "    'MAE': metrics.mean_absolute_error(y_test, y_pred),\n",
    "    'MSE': metrics.mean_squared_error(y_test, y_pred),\n",
    "    'RMSE': np.sqrt(metrics.mean_squared_error(y_test, y_pred)),\n",
    "    'r squared': metrics.r2_score(y_test, y_pred),\n",
    "    'explained variance score': metrics.explained_variance_score(y_test, y_pred),\n",
    "    'max error': metrics.max_error(y_test, y_pred)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model #7: based on (1) number of sources and (2) number of meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = '#7'\n",
    "predictors = ['number of meanings', 'sources (Fr)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = confidence_df[['number of meanings', 'sources (Fr)']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = confidence_df['confidence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()  \n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred}, index=range(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Mean absolute error should be within the 10% range of the mean confidence value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_conf_df['confidence'].mean() # mean confidence value = 0.6191 >> MAE should be not greater than 0.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: R squared is in the interval (0, 1), the greater the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.explained_variance_score(y_test, y_pred) # maximum value is 1 and is desirable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.max_error(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.median_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add result to a dictionary\n",
    "models_results.append({\n",
    "    'model': model, \n",
    "    'predictors': predictors,\n",
    "    'additional': 'random_state=0',\n",
    "    'MAE': metrics.mean_absolute_error(y_test, y_pred),\n",
    "    'MSE': metrics.mean_squared_error(y_test, y_pred),\n",
    "    'RMSE': np.sqrt(metrics.mean_squared_error(y_test, y_pred)),\n",
    "    'r squared': metrics.r2_score(y_test, y_pred),\n",
    "    'explained variance score': metrics.explained_variance_score(y_test, y_pred),\n",
    "    'max error': metrics.max_error(y_test, y_pred)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model #8: based on (1) number of sources and (2) expected confidence: meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = '#8'\n",
    "predictors = ['expected confidence: nr of meanings', 'sources (Fr)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = confidence_df[['expected confidence: nr of meanings', 'sources (Fr)']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = confidence_df['confidence'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = LinearRegression()  \n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred}, index=range(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 1: Mean absolute error should be within the 10% range of the mean confidence value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_df['confidence'].mean() # mean confidence value = 0.6191 >> MAE should be not greater than 0.06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test 2: R squared is in the interval (0, 1), the greater the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.explained_variance_score(y_test, y_pred) # maximum value is 1 and is desirable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.max_error(y_test, y_pred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.median_absolute_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add result to a dictionary\n",
    "models_results.append({\n",
    "    'model': model, \n",
    "    'predictors': predictors,\n",
    "    'additional': 'random_state=42',\n",
    "    'MAE': metrics.mean_absolute_error(y_test, y_pred),\n",
    "    'MSE': metrics.mean_squared_error(y_test, y_pred),\n",
    "    'RMSE': np.sqrt(metrics.mean_squared_error(y_test, y_pred)),\n",
    "    'r squared': metrics.r2_score(y_test, y_pred),\n",
    "    'explained variance score': metrics.explained_variance_score(y_test, y_pred),\n",
    "    'max error': metrics.max_error(y_test, y_pred)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.pprint(models_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance = pd.DataFrame(models_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_performance.to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_styles.iloc[:,:5].to_clipboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it seems, the best two models to predict the confidence of a word annotation so far are:\n",
    "\n",
    "1) the model based on the expected confidence for the word with the same number of meanings, the expected confidence of the annotator, and the confidence that the annotator showed when annotating the test word 'virtus'\n",
    "\n",
    "2) the model based on the count of patterns and number of meaning clusters / number of meanings for the word\n",
    "\n",
    "It seems that the annotators' personal differences in confidence and 'annotation style' do not contribute as much to the overall confidence of the annotation as the qualities of the word that was annotated. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
